{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Tabular Q Learning\n",
    "\n",
    "Tabular Q Learning을 실습해봅니다.\n",
    "- 모든 state의 value function을 table에 저장하고 테이블의 각 요소를 Q Learning으로 업데이트 하는 것으로 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab 용 package 설치 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:  1.8.0\n",
      "gym version:  0.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "# from gym.wrappers import Monitor\n",
    "\n",
    "np.random.seed(777)\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "print(\"tensorflow version: \", tf.__version__)\n",
    "print(\"gym version: \", gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "\n",
    "**[state]**\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    \n",
    "**[action]**\n",
    "\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Load Environment\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "# init envrionmnet\n",
    "env.reset()\n",
    "# only 'Right' action agent\n",
    "for _ in range(5):\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Lake (not Slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_frozen_lake_not_slippery(name):\n",
    "    from gym.envs.registration import register\n",
    "    register(\n",
    "        id=name,\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "\n",
    "register_frozen_lake_not_slippery('FrozenLakeNotSlippery-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "env.reset()\n",
    "env.render()\n",
    "'''\n",
    "env.step()을 이용해서 Goal까지 직접 이동해보세요.\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "'''\n",
    "env.step(0); env.render()\n",
    "# env.step(); env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "**Pseudo code**  \n",
    "<img src=\"./img/qlearning_pseudo.png\" width=\"80%\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# epsilon greedy policy\n",
    "\n",
    "def epsilon_greedy_action(epsilon, n_action, state, q_table):\n",
    "    \n",
    "        # 구현해보세요.\n",
    "        # if epsilon이 random 값보다 클때\n",
    "            # random action\n",
    "        # else\n",
    "            # 가장 큰 Q값을 갖는 action을 고른다.\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# epsilon greedy test\n",
    "\n",
    "epsilon = 0\n",
    "q_table = np.array([[1,0,0,0],\n",
    "                    [0,0,0,1],\n",
    "                    [0,1,0,0]])\n",
    "for state in range(3):\n",
    "    action = epsilon_greedy_action(epsilon, 4, state, q_table)\n",
    "    print(\"state: {}    action: {}\".format(state, action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-value update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_update(q_table, state, next_state, action, reward, alpha, gamma):\n",
    "    \n",
    "    # 구현해보세요.\n",
    "    # update 수식은 pseudo code 참조\n",
    "    # q_table[s, a] = q_table[s, a] + TD error\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "q_table = np.array([[0,0,0,0],\n",
    "                    [0,1,0,0]], dtype=np.float)\n",
    "print(\"start\\n\", q_table)\n",
    "\n",
    "reward = 1.0\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"update {}\".format(i))\n",
    "    q_table = q_update(q_table, 0, 1, 2, reward, alpha, gamma)\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal에 도착하기 위해 생각해야 하는것\n",
    "1. Goal에 한번이라도 도착해야만 reward가 나와서 update 된다 $\\rightarrow$ goal에 어떻게 가게 할까?\n",
    "2. hole에 빠졌을 때 episode가 끝나긴 하지만 reward에 차이는 없다. $\\rightarrow$ hole에 빠져서 끝나면 negative reward를 주도록 한다.\n",
    "3. 학습이 잘 되어도 epsilon 만큼의 확률로 random action을 한다. $\\rightarrow$ 학습이 진행될수록 epsilon을 줄인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tabular_Q_agent:\n",
    "    def __init__(self, q_table, n_action, epsilon, alpha, gamma):\n",
    "        self.q_table = q_table\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.n_action = n_action\n",
    "    \n",
    "    def get_action(self, state):\n",
    "    \n",
    "        # 구현해보세요. (e-greedy policy)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def q_update(self, state, next_state, action, reward):\n",
    "    \n",
    "        # 구현해보세요.\n",
    "        # update 수식은 pseudo code 참조\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-8f6fc8c8f91d>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-8f6fc8c8f91d>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    agent =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "\n",
    "EPISODE = 500\n",
    "epsilon = 0.9\n",
    "alpha = 0.8 # learning rate\n",
    "gamma = 0.9 # discount factor\n",
    "n_action = \n",
    "\n",
    "rlist = []\n",
    "slist = []\n",
    "\n",
    "is_render = False\n",
    "\n",
    "# initialize Q-Table \n",
    "q_table = np.random.rand(env.observation_space.n, env.action_space.n)\n",
    "print(\"Q table size: \", q_table.shape)\n",
    "\n",
    "# agent 생성\n",
    "agent = \n",
    "\n",
    "# Epiode 수만큼 반복\n",
    "for e in range(EPISODE):\n",
    "    state = env.reset()\n",
    "    print(\"[Episode {}]\".format(e))\n",
    "    if is_render:\n",
    "        env.render()\n",
    "    \n",
    "    total_reward = 0\n",
    "    goal = 0\n",
    "    done = False\n",
    "    limit = 0\n",
    "    \n",
    "    # 게임이 끝날때까지 반복 또는 100번 step할 때까지 반복\n",
    "    while not done and limit < 100:\n",
    "        # 1. select action by e-greedy policy\n",
    "        # e-greedy로 action을 선택.\n",
    "            \n",
    "        # 2. do action and go to next state\n",
    "        # env.step()을 사용해 1 step 이동 후 next state와 reward, done 값을 받아옴.\n",
    "        \n",
    "        # 2.1. hole 에 빠졌을 때 (-) reward를 받도록 함.\n",
    "        if reward == 1.0:\n",
    "            print(\"GOAL\")\n",
    "            goal = 1\n",
    "        # hole에 빠졌을 경우 -1 reward\n",
    "        elif done:\n",
    "            reward = reward - 1\n",
    "        \n",
    "        # 3. Q update\n",
    "        # Q table에서 현재 state의 Q값을 update 한다.\n",
    "        \n",
    "        slist.append(state)\n",
    "        state = next_state\n",
    "        \n",
    "        total_reward += reward\n",
    "        limit += 1\n",
    "        \n",
    "    print(slist)\n",
    "    slist = []\n",
    "    print(\"total reward: \", total_reward)\n",
    "    rlist.append(goal)\n",
    "    \n",
    "print(\"성공한 확률\" + str(sum(rlist) / EPISODE) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "print(agent.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "limit = 0\n",
    "\n",
    "agent.epsilon = 0.0\n",
    "while not done and limit < 30:\n",
    "    action = agent.get_action(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    state = next_state\n",
    "    limit += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_rl",
   "language": "python",
   "name": "tf_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
