{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Neuralnet Q Learning example\n",
    "\n",
    "Neuralnet Q Learning을 실습해봅니다.\n",
    "- 신경망의 parameter(weight)를 업데이트 함에 따라 state에 대한 Q value가 변화합니다.\n",
    "- Q Learning의 TD error를 loss function으로 하여 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collab 용 package 설치 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(285)\n",
    "tf.set_random_seed(285)\n",
    "\n",
    "print(\"tensorflow version: \", tf.__version__)\n",
    "print(\"gym version: \", gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "\n",
    "**[state]**\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    \n",
    "**[action]**\n",
    "\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Lake (not Slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def register_frozen_lake_not_slippery(name):\n",
    "    from gym.envs.registration import register\n",
    "    register(\n",
    "        id=name,\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "\n",
    "register_frozen_lake_not_slippery('FrozenLakeNotSlippery-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "print(\"State_size : \", state_size)\n",
    "print(\"Action_size: \",action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning using Neural Network  \n",
    "**Update 식**  \n",
    "  \n",
    "$J(w) = \\mathbb{E}_{\\pi}[(target - \\hat q(S,A,w))^2]$  \n",
    "  \n",
    "$ \\begin{align} \\Delta w & = - \\frac{1}{2} \\alpha \\nabla_w J(w)\n",
    "\\\\ & = \\alpha (R_{t+1} + \\gamma max(\\hat q(S_{t+1},a ,w)) - \\hat q(S_{t},A_{t},w))\\nabla_w \\hat q(S_{t},A_{t},w) \\end{align}$\n",
    "\n",
    "### 학습 순서\n",
    "1. 초기 state 받음 (env.reset())\n",
    "2. action 선택 (e - greedy policy)\n",
    "3. 선택한 action으로 다음 state로 이동 (env.step())\n",
    "4. 다음 state와 reward를 이용해 update식 작성\n",
    "5. 신경망 업데이트\n",
    "6. 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 코드 흐름\n",
    "1. 각 연산자에 대한 그래프를 구성한다.\n",
    "2. 실제 데이터를 그래프에 할당하면서 전체 그래프를 실행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder 선언\n",
    "# state\n",
    "inputs = tf.placeholder(shape=[1], dtype=tf.int64)\n",
    "# state에 대한 action\n",
    "input_action = tf.placeholder(shape=[1], dtype=tf.int64)\n",
    "# Loss 식의 target\n",
    "target = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "\n",
    "layers = tf.contrib.layers\n",
    "\n",
    "# 신경망 구성 함수\n",
    "# one-hot vector : 입력 1을 단순한 숫자로 받는 것보다 [1, 0, 0, 0] 처럼 encoding된 값으로 바꾸어 받는 것이\n",
    "#                       학습에 유리하다. 모든 입력이 크기에 관계없이 동등해지게 된다.\n",
    "# tf.one_hot( 입력, one-hot size )\n",
    "def build_network(inputs):   \n",
    "    with tf.variable_scope('q_net'):\n",
    "        # 빈칸 {} 을 지우고 채워주세요.\n",
    "        input_onehot = tf.one_hot({}, {}, dtype=tf.float32)\n",
    "        fc1 = layers.fully_connected(inputs={},\n",
    "                                                  num_outputs={},\n",
    "                                                  activation_fn=None)\n",
    "    return fc1\n",
    "\n",
    "# 신경망 구성\n",
    "q_value = build_network(inputs)\n",
    "\n",
    "# 현재 action에 대한 Q_value 구하는 연산\n",
    "# q_value = [1, 2, 3, 4] curr_action = [0, 1, 0, 0] --(원소 곱)--> [0, 2, 0, 0] --(sum)--> [2]\n",
    "curr_action = tf.one_hot(input_action, action_size)\n",
    "curr_q_value = tf.reduce_sum(tf.multiply(q_value, curr_action))\n",
    "\n",
    "# Loss 함수 구성\n",
    "# 직접 구현해보세요. ( learning_rate = 0.1 )\n",
    "# 참고) 제곱 : tf.square()     \n",
    "#          optimizer : tf.train.GradientDescentOptimizer( learning_rate )\n",
    "loss_op = \n",
    "opt = \n",
    "train_op = opt.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing a graph in a tf.Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session 열기\n",
    "sess = tf.Session()\n",
    "\n",
    "# 변수 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 변수(파라미터) 확인\n",
    "for var in tf.trainable_variables('q_net'):\n",
    "    print(var)\n",
    "    print(sess.run(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action select using Q value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 state\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1])\n",
    "print(\"현재 state:\", state)\n",
    "\n",
    "# 현재 state에 대한 Q-value\n",
    "# 빈칸 {}을 채워보세요.\n",
    "# 참고) sess.run( \"Q-value를 구하는 신경망 그래프\", feed_dict={inputs: \"신경망 입력\"} )\n",
    "curr_q = sess.run({}, feed_dict={inputs: {} })\n",
    "print(\"현재 state의 Q value:\", curr_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# action 선택 ( greedy )\n",
    "# 직접 구현해보세요.\n",
    "action = \n",
    "print(\"선택한 action: \", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선택한 Action으로 다음 State, Reward 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action을 이용해 env.step하기\n",
    "# 빈칸 {} 을 채워보세요.\n",
    "next_state, reward, done, _ = env.step({})\n",
    "next_state = np.reshape(next_state, [1])\n",
    "print(next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update를 위한 (미래)보상 값(target) 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "if done:\n",
    "    next_q_value = np.array([reward])\n",
    "else:\n",
    "    # 직접 작성해보세요.\n",
    "    # 위 수식 참고.\n",
    "    # 참고) R + gamma * next state의 q-value 중 max\n",
    "    next_q_value = \n",
    "print(next_q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action = np.reshape(action, [1])\n",
    "\n",
    "# train_op를 sess.run 하여 학습 실행.\n",
    "# 빈칸 {} 을 채워보세요.\n",
    "loss, _ = sess.run([loss_op, train_op], feed_dict={inputs: {}, target: {}, input_action: {}})\n",
    "print(loss)\n",
    "\n",
    "for var in tf.trainable_variables('q_net'):\n",
    "    print(var)\n",
    "    print(sess.run(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rlist = []\n",
    "slist = []\n",
    "epsilon_list = []\n",
    "\n",
    "EPISODE = 2000\n",
    "gamma = 0.99\n",
    "\n",
    "update_count = 0\n",
    "loss_list = []\n",
    "\n",
    "# Episode 수만큼 반복\n",
    "for step in range(EPISODE):\n",
    "    epsilon = 1. / ((step/50)+10)\n",
    "    epsilon_list.append(epsilon)\n",
    "    \n",
    "    # 초기 state\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1])\n",
    "    print(\"[Episode {}]\".format(step))\n",
    "    total_reward = 0\n",
    "    limit = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done and limit < 99:\n",
    "        # 위에서 했던 코드를 참조하여 아래 학습 코드를 작성해보세요.\n",
    "        # 현재 state의 Q value불러오기\n",
    "        curr_q = \n",
    "        \n",
    "        # e-greedy policy로 action 선택\n",
    "        if epsilon > np.random.random():\n",
    "            # random\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # greedy\n",
    "            action =  \n",
    "            \n",
    "        # 선택한 action으로 env.step 하기\n",
    "        next_state, reward, done, _ = \n",
    "        next_state = np.reshape(next_state, [1])\n",
    "                          \n",
    "        if reward == 1.0:\n",
    "            print(\"GOAL\")\n",
    "        \n",
    "        # 업데이트를 위한 (미래)보상값 반환\n",
    "        # episode가 끝났다면\n",
    "        if done:\n",
    "            next_q_value = \n",
    "        # 끝나지 않았다면\n",
    "        else:\n",
    "            next_q_value = \n",
    "        \n",
    "        # Q update\n",
    "        action = np.reshape(action, [1])\n",
    "        loss, _ = \n",
    "        \n",
    "        loss_list.append(loss)\n",
    "        update_count += 1\n",
    "        \n",
    "        slist.append(state.item())\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        limit += 1\n",
    "        \n",
    "    print(slist)\n",
    "    slist = []\n",
    "    print(\"total reward: \", total_reward)\n",
    "    rlist.append(total_reward)\n",
    "    \n",
    "print(\"성공한 확률\" + str(sum(rlist) / EPISODE) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in tf.trainable_variables('q_net'):\n",
    "    print(var)\n",
    "    print(sess.run(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon 변화 그래프\n",
    "steps = np.arange(EPISODE)\n",
    "plt.title('Epsilon values')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('$\\\\epsilon$')\n",
    "plt.plot(steps, epsilon_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 변화 그래프\n",
    "update_count = np.arange(update_count)\n",
    "plt.title('Loss values')\n",
    "plt.xlabel('Update Count')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(update_count, loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = np.reshape(state, [1])\n",
    "done = False\n",
    "limit = 0\n",
    "\n",
    "epsilon = 0.0\n",
    "while not done and limit < 30:\n",
    "    # 학습된 신경망을 테스트하는 코드를 작성해보세요.\n",
    "    curr_q = \n",
    "    action = \n",
    "    next_state, reward, done, _ =\n",
    "    next_state = np.reshape(next_state, [1])\n",
    "    \n",
    "    env.render()\n",
    "    state = next_state\n",
    "    limit += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sonic)",
   "language": "python",
   "name": "sonic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
